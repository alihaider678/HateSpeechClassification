{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:/STUDY/Projects/hatespeechclassification/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',\n",
      "       'class', 'tweet'],\n",
      "      dtype='object')\n",
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
     ]
    }
   ],
   "source": [
    "# Inspect columns\n",
    "print(df.columns)\n",
    "\n",
    "# Display first few rows to ensure data is loaded correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Drop unnecessary columns and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_9484\\422446586.py:25: SyntaxWarning: invalid escape sequence '\\['\n",
      "  words = re.sub('\\[.*?\\]', '', words)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_9484\\422446586.py:26: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  words = re.sub('https?://\\S+|www\\.\\S+', '', words)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_9484\\422446586.py:30: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  words = re.sub('\\w*\\d\\w*', '', words)\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to drop if they exist\n",
    "columns_to_drop = ['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither']\n",
    "\n",
    "# Drop columns that exist in the DataFrame\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# Ensure 'class' column exists before proceeding\n",
    "if 'class' in df.columns:\n",
    "    # Replace class values to binary (0 and 1)\n",
    "    df['class'] = df['class'].replace({0: 1, 2: 0})\n",
    "\n",
    "    # Rename the 'class' column to 'label'\n",
    "    df = df.rename(columns={'class': 'label'})\n",
    "else:\n",
    "    print(\"Error: 'class' column not found in the DataFrame.\")\n",
    "    # You can choose to raise an error or handle it as needed\n",
    "    raise KeyError(\"'class' column not found in the DataFrame\")\n",
    "\n",
    "# Data cleaning function\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "stopword = set(stopwords.words('english'))\n",
    "\n",
    "def data_cleaning(words):\n",
    "    words = str(words).lower()\n",
    "    words = re.sub('\\[.*?\\]', '', words)\n",
    "    words = re.sub('https?://\\S+|www\\.\\S+', '', words)\n",
    "    words = re.sub('<.*?>+', '', words)\n",
    "    words = re.sub('[%s]' % re.escape(string.punctuation), '', words)\n",
    "    words = re.sub('\\n', '', words)\n",
    "    words = re.sub('\\w*\\d\\w*', '', words)\n",
    "    words = [word for word in words.split(' ') if word not in stopword]\n",
    "    words = \" \".join(words)\n",
    "    words = [stemmer.stem(word) for word in words.split(' ')]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(data_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['tweet']\n",
    "y = df['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50000\n",
    "max_len = 300\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_matrix = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\STUDY\\Projects\\hatespeechclassification\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 613ms/step - accuracy: 0.8220 - loss: 0.4344 - val_accuracy: 0.9333 - val_loss: 0.1657\n",
      "Epoch 2/5\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 650ms/step - accuracy: 0.9375 - loss: 0.1660 - val_accuracy: 0.9494 - val_loss: 0.1268\n",
      "Epoch 3/5\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 694ms/step - accuracy: 0.9582 - loss: 0.1126 - val_accuracy: 0.9519 - val_loss: 0.1235\n",
      "Epoch 4/5\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 721ms/step - accuracy: 0.9691 - loss: 0.0854 - val_accuracy: 0.9556 - val_loss: 0.1207\n",
      "Epoch 5/5\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 713ms/step - accuracy: 0.9743 - loss: 0.0748 - val_accuracy: 0.9435 - val_loss: 0.1470\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(sequences_matrix, y_train, batch_size=128, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m194/194\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - accuracy: 0.9414 - loss: 0.1600\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "test_sequences_matrix = pad_sequences(test_sequences, maxlen=max_len)\n",
    "accr = model.evaluate(test_sequences_matrix, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
